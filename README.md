# ğŸŒŸ OWE-YOR: Leveraging Transformer-Based Models for Yoruba Proverb Classification

**OWE-YOR** is a state-of-the-art NLP project that focuses on the classification of **Yoruba proverbs** using Transformer-based models. It distinguishes between proverbs and non-proverbs, leveraging modern AI to preserve Yoruba language and culture.  

---

## ğŸš€ Project Workflow

- [Introduction](#introduction)  
- [Features](#features)  
- [Dataset](#dataset)  
- [Models](#models)  
- [Installation](#installation)  
- [Usage](#usage)  
- [Results](#results)  
- [Future Work](#future-work)  
- [Team](#team)  
- [License](#license)  
- [Acknowledgements](#acknowledgements)  

---

## ğŸ“ Introduction

Yoruba proverbs are rich in cultural and linguistic meaning. Automating their detection is challenging due to limited annotated data. **OWE-YOR** uses Transformer-based models like **AfroLM** and **mBERT** to capture semantic and contextual subtleties, outperforming traditional machine learning models like Naive Bayes.  

**Goals of this project:**  
- Build an automated Yoruba proverb detection system âœ…  
- Compare traditional ML and Transformer-based approaches âš¡  
- Contribute to AI tools for African languages ğŸŒ  

---

## âœ¨ Features

- Detects Yoruba proverbs vs. non-proverbs  
- Uses **Transformer-based models** for high accuracy  
- Provides a **Streamlit demo** for interactive testing  
- Easily extensible to other low-resource African languages  

---

## ğŸ“‚ Dataset

- **Proverbs:** labeled `1`  
- **Non-proverbs:** labeled `0`  
- Data is **cleaned and preprocessed** for optimal training.  

> Dataset link: [Add your dataset link here]  

---

## ğŸ¤– Models

- **Multinomial Naive Bayes** â€“ baseline performance  
- **AfroLM** and **mBERT** â€“ Transformer-based classification  

**Performance Comparison Example:**  

| Model                   | Accuracy |
|-------------------------|---------|
| Multinomial Naive Bayes | 85%     |
| AfroLM / mBERT          | 92â€“95%  |

> Transformers better capture semantic context in Yoruba proverbs ğŸŒŸ  

---

## âš™ï¸ Installation

```bash
git clone https://github.com/<your-username>/OWE-YOR.git
cd OWE-YOR
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt


ğŸŒ Demo

Try the web app live on Hugging Face Spaces:

ğŸ¯ OWE-YOR: Yoruba Proverb Classifier

ğŸ“Š Results

High accuracy in classifying Yoruba proverbs vs. non-proverbs

Transformers outperform traditional ML in low-resource languages

Confusion matrix and metrics available in results/ folder

ğŸ”® Future Work

Expand to other African languages ğŸŒ

Integrate into culturally-aware conversational AI systems ğŸ’¬

Explore tone-aware models for tonal languages like Yoruba ğŸµ

ğŸ‘¥ Team

Olusanya Joy â€“ GitHub

Your Partner's Name â€“ GitHub

ğŸ“œ License

MIT License. See LICENSE
 for details.

ğŸ™ Acknowledgements

Kwame AI and FriendnPal for resources

Hugging Face for pretrained models (AfroLM, mBERT)

Open Speech and Language Resources (OSLR) for dataset support
